alg_name: ROME
attn_module_tmp: transformer.h.{}.attn
clamp_norm_factor: 4
context_template_length_params:
- - 5
  - 10
- - 10
  - 10
device: 0
fact_token: subject_last
fp16: false
kl_factor: 0.0625
layer_module_tmp: transformer.h.{}
layers:
- 17
lm_head_module: transformer.wte
ln_f_module: transformer.ln_f
mlp_module_tmp: transformer.h.{}.mlp
model_name: gpt2-xl
model_parallel: false
mom2_adjustment: false
mom2_dataset: wikipedia
mom2_dtype: float32
mom2_n_samples: 100000
noise_level: 0.13462980464100838
rewrite_module_tmp: transformer.h.{}.mlp.c_proj
stats_dir: ./data/stats
v_loss_layer: 47
v_lr: 5e-1
v_num_grad_steps: 20
v_weight_decay: 0.5
